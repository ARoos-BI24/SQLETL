{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8d3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Setup\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = 'C:/Users/anton/.conda/envs/conda-env/Library/lib/jvm'\n",
    "os.environ['HADOOP_HOME'] = 'C:/Hadoop/hadoop-3.3.6'\n",
    "os.environ['PATH'] += ';C:/Hadoop/hadoop-3.3.6/bin'\n",
    "os.environ['SPARK_HOME'] = r\"C:\\Users\\anton\\.conda\\envs\\conda-env\\Lib\\site-packages\\pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baabb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    when, col, row_number, current_timestamp, date_format, from_utc_timestamp, lit, regexp_replace\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pathlib import Path\n",
    "import pyodbc\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ETL_Assignment2_Warehouse\") \\\n",
    "    .config(\"spark.jars\", r\"C:\\Spark\\sqljdbc_12.10\\enu\\jars\\mssql-jdbc-12.10.0.jre8.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "conn = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=PC-W11;UID=admin;PWD=sql\", autocommit=True)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to check if the database exists and create it if it doesn't\n",
    "create_db_query = \"\"\"\n",
    "IF NOT EXISTS (SELECT name FROM master.dbo.sysdatabases WHERE name = 'ETL_Assignment2_Warehouse')\n",
    "BEGIN\n",
    "    CREATE DATABASE ETL_Assignment2_Warehouse\n",
    "    PRINT 'Database ETL_Assignment2_Warehouse created successfully.'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    PRINT 'Database ETL_Assignment2_Warehouse already exists.'\n",
    "END\n",
    "\"\"\"\n",
    "cursor.execute(create_db_query)\n",
    "conn.close()\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"base_path\": \"data\",\n",
    "    \"db_url\": \"jdbc:sqlserver://PC-W11:1433;databaseName=ETL_Assignment2_Warehouse;user=admin;password=sql;encrypt=false;trustServerCertificate=true\",\n",
    "    \"pyodbc_conn\": \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=PC-W11;DATABASE=ETL_Assignment2_Warehouse;UID=admin;PWD=sql\",\n",
    "    \"files\": {\n",
    "        \"account\": {\n",
    "            \"csv\": \"data/Raw_Loan_Account.csv\",\n",
    "            \"parquet\": \"data/Loan_Account_cleansed.parquet\",\n",
    "            \"table\": \"Loan_Account_cleansed\"\n",
    "        },\n",
    "        \"balance\": {\n",
    "            \"csv\": \"data/Raw_Loan_Balance.csv\",\n",
    "            \"parquet\": \"data/Loan_Balance_cleansed.parquet\",\n",
    "            \"table\": \"Loan_Balance_cleansed\"\n",
    "        },\n",
    "        \"transactions\": {\n",
    "            \"csv\": \"data/Raw_Loan_Transaction.csv\",\n",
    "            \"parquet\": \"data/Loan_Transaction_cleansed.parquet\",\n",
    "            \"table\": \"Loan_Transaction_cleansed\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Schema definitions\n",
    "account_schema = StructType([\n",
    "    StructField(\"LoanAccountId\", IntegerType(), False),\n",
    "    StructField(\"SourceId\", IntegerType(), False),\n",
    "    StructField(\"AccountNumber\", StringType(), False),\n",
    "    StructField(\"IBAN\", StringType(), True),\n",
    "    StructField(\"BBAN\", StringType(), True),\n",
    "    StructField(\"AccountCurrencyId\", IntegerType(), False),\n",
    "    StructField(\"AccountCurrency\", StringType(), False),\n",
    "    StructField(\"OrganizationId\", IntegerType(), False),\n",
    "    StructField(\"OrganizationName\", StringType(), False),\n",
    "    StructField(\"ChannelID\", IntegerType(), True),\n",
    "    StructField(\"BrokerId\", IntegerType(), False),\n",
    "    StructField(\"OpenDateId\", IntegerType(), False),\n",
    "    StructField(\"OpenDate\", DateType(), False),\n",
    "    StructField(\"CancelledDateId\", IntegerType(), True),\n",
    "    StructField(\"CancelledDate\", DateType(), True),\n",
    "    StructField(\"ValueDate\", DateType(), False),\n",
    "    StructField(\"MaturityDate\", DateType(), True),\n",
    "    StructField(\"ProductId\", IntegerType(), False),\n",
    "    StructField(\"Product\", StringType(), False),\n",
    "    StructField(\"InvoiceDay\", IntegerType(), False),\n",
    "    StructField(\"CurrentInstallmentAmount\", StringType(), False),\n",
    "    StructField(\"CurrentInvoiceFee\", StringType(), False),\n",
    "    StructField(\"RepaymentRate\", StringType(), True),\n",
    "    StructField(\"NextInvoiceDate\", DateType(), False),\n",
    "    StructField(\"CalculatedMaturityDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "balance_schema = StructType([\n",
    "    StructField(\"LoanAccountBalanceId\", IntegerType(), False),\n",
    "    StructField(\"SourceId\", IntegerType(), False),\n",
    "    StructField(\"BalanceDateId\", IntegerType(), False),\n",
    "    StructField(\"LoanAccountId\", IntegerType(), False),\n",
    "    StructField(\"ProductId\", IntegerType(), False),\n",
    "    StructField(\"AccountCurrencyId\", IntegerType(), False),\n",
    "    StructField(\"AccountStatusId\", IntegerType(), False),\n",
    "    StructField(\"NumOfTransactions\", IntegerType(), False),\n",
    "    StructField(\"NetTransactionAmount\", StringType(), False),\n",
    "    StructField(\"NetTransactionAmountSek\", StringType(), False),\n",
    "    StructField(\"AccruedInterest\", StringType(), False),\n",
    "    StructField(\"AccruedInterestSEK\", StringType(), False),\n",
    "    StructField(\"Balance\", StringType(), False),\n",
    "    StructField(\"BalanceSek\", StringType(), False),\n",
    "    StructField(\"LTV\", IntegerType(), False),\n",
    "    StructField(\"PrecedingId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"LoanAccountTransactionId\", IntegerType(), False),\n",
    "    StructField(\"SourceId\", IntegerType(), False),\n",
    "    StructField(\"TransactionDateId\", IntegerType(), False),\n",
    "    StructField(\"ValueDateId\", IntegerType(), False),\n",
    "    StructField(\"EntryDateID\", IntegerType(), False),\n",
    "    StructField(\"LoanAccountId\", IntegerType(), False),\n",
    "    StructField(\"TransactionTypeId\", IntegerType(), False),\n",
    "    StructField(\"TransactionStatus\", StringType(), True),\n",
    "    StructField(\"RectifyStatus\", StringType(), True),\n",
    "    StructField(\"TransactionCurrencyId\", IntegerType(), False),\n",
    "    StructField(\"TransactionAmount\", StringType(), False),\n",
    "    StructField(\"TransactionAmountSEK\", StringType(), False),\n",
    "    StructField(\"CounterpartClearingNumber\", StringType(), True),\n",
    "    StructField(\"CounterPartBic\", StringType(), True),\n",
    "    StructField(\"CounterPartIban\", StringType(), True),\n",
    "    StructField(\"TransactionReference\", StringType(), True),\n",
    "    StructField(\"ExchangeRateId\", IntegerType(), False),\n",
    "    StructField(\"TransactionText\", StringType(), True),\n",
    "    StructField(\"AccountServicerReference\", StringType(), True),\n",
    "    StructField(\"CounterPartId\", IntegerType(), True),\n",
    "    StructField(\"CounterPartAccountNumber\", StringType(), True),\n",
    "    StructField(\"CounterPartBankName\", StringType(), True),\n",
    "    StructField(\"TransactionDateTime\", TimestampType(), True),\n",
    "    StructField(\"IsDirectDebit\", IntegerType(), True),\n",
    "    StructField(\"GLAccount\", StringType(), True),\n",
    "    StructField(\"EventName\", StringType(), True),\n",
    "    StructField(\"InvoiceId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"Configuration and schema loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c498692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Utility Functions\n",
    "def convert_european_decimals(df, decimal_columns):\n",
    "    \"\"\"Convert European decimal format (comma) to US format (dot) and cast to decimal\"\"\"\n",
    "    converted_df = df\n",
    "\n",
    "    for col_name in decimal_columns:\n",
    "        if col_name in df.columns:\n",
    "            # Replace comma with dot and handle null/empty values\n",
    "            converted_df = converted_df.withColumn(\n",
    "                col_name,\n",
    "                when(\n",
    "                    (col(col_name).isNull()) |\n",
    "                    (col(col_name) == \"\") |\n",
    "                    (col(col_name) == \"NULL\"),\n",
    "                    lit(0.0)\n",
    "                ).otherwise(regexp_replace(col(col_name), \",\", \".\"))\n",
    "            )\n",
    "            converted_df = converted_df.withColumn(\n",
    "                col_name,\n",
    "                converted_df[col_name].cast(DecimalType(12, 5))\n",
    "            )\n",
    "\n",
    "    return converted_df\n",
    "\n",
    "def check_table_exists(table_name):\n",
    "    \"\"\"Check if table exists in database\"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(CONFIG[\"pyodbc_conn\"])\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM INFORMATION_SCHEMA.TABLES\n",
    "            WHERE TABLE_NAME = '{table_name}'\n",
    "        \"\"\")\n",
    "        exists = cursor.fetchone()[0] > 0\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return exists\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking table existence: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_account_table(table_name):\n",
    "    \"\"\"Create loan account cleansed table\"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(CONFIG[\"pyodbc_conn\"])\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            LoanAccountIdentityId INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            LoanAccountId INT NOT NULL,\n",
    "            SourceId INT NOT NULL DEFAULT 8,\n",
    "            AccountNumber NVARCHAR(50) NOT NULL,\n",
    "            IBAN NVARCHAR(34),\n",
    "            BBAN NVARCHAR(34),\n",
    "            AccountCurrencyId INT NOT NULL,\n",
    "            AccountCurrency NVARCHAR(3) NOT NULL CHECK (LEN(AccountCurrency) = 3),\n",
    "            OrganizationId INT NOT NULL,\n",
    "            OrganizationName NVARCHAR(50) NOT NULL,\n",
    "            ChannelID INT,\n",
    "            BrokerId INT NOT NULL,\n",
    "            OpenDateId INT NOT NULL,\n",
    "            OpenDate DATE NOT NULL,\n",
    "            CancelledDateId INT,\n",
    "            CancelledDate DATE,\n",
    "            ValueDate DATE NOT NULL,\n",
    "            MaturityDate DATE,\n",
    "            ProductId INT NOT NULL,\n",
    "            Product NVARCHAR(30) NOT NULL,\n",
    "            InvoiceDay TINYINT NOT NULL DEFAULT 14,\n",
    "            CurrentInstallmentAmount DECIMAL(12,2) NOT NULL,\n",
    "            CurrentInvoiceFee DECIMAL(12,2) NOT NULL,\n",
    "            RepaymentRate NVARCHAR(50),\n",
    "            NextInvoiceDate DATE NOT NULL,\n",
    "            CalculatedMaturityDate DATE,\n",
    "            IsActive AS (CASE WHEN CancelledDate IS NULL THEN 1 ELSE 0 END) PERSISTED,\n",
    "            CreatedDate DATETIME DEFAULT GETDATE(),\n",
    "            UpdatedDate DATE DEFAULT GETDATE(),\n",
    "            UpdatedTime TIME DEFAULT GETDATE()\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_sql)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(f\"Table {table_name} created successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_balance_table(table_name):\n",
    "    \"\"\"Create loan balance cleansed table\"\"\"\n",
    "    print(f\"Creating database table {table_name}...\")\n",
    "    try:\n",
    "        conn = pyodbc.connect(CONFIG[\"pyodbc_conn\"])\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            LoanAccountBalanceIdentityId INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            LoanAccountBalanceId INT NOT NULL,\n",
    "            SourceId INT NOT NULL,\n",
    "            BalanceDateId INT NOT NULL,\n",
    "            BalanceDate AS CONVERT(DATE, CAST(BalanceDateId AS CHAR(8)), 112) PERSISTED,\n",
    "            LoanAccountId INT NOT NULL,\n",
    "            ProductId INT NOT NULL,\n",
    "            AccountCurrencyId INT NOT NULL,\n",
    "            AccountStatusId INT NOT NULL,\n",
    "            NumOfTransactions INT NOT NULL DEFAULT 0,\n",
    "            NetTransactionAmount DECIMAL(12,2) DEFAULT 0,\n",
    "            NetTransactionAmountSek DECIMAL(12,2) DEFAULT 0,\n",
    "            AccruedInterest DECIMAL(12,2) DEFAULT 0,\n",
    "            AccruedInterestSEK DECIMAL(12,5) DEFAULT 0,\n",
    "            Balance DECIMAL(12,2) NULL DEFAULT 0,\n",
    "            BalanceSek DECIMAL(12,5) DEFAULT 0,\n",
    "            LTV INT NOT NULL DEFAULT 0,\n",
    "            PrecedingId INT,\n",
    "            CreatedDate DATETIME DEFAULT GETDATE(),\n",
    "            UpdatedDate DATE DEFAULT GETDATE(),\n",
    "            UpdatedTime TIME DEFAULT GETDATE()\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_sql)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(f\"Table {table_name} created successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_transaction_table(table_name):\n",
    "    \"\"\"Create loan transaction cleansed table\"\"\"\n",
    "    print(f\"Creating database table {table_name}...\")\n",
    "    try:\n",
    "        conn = pyodbc.connect(CONFIG[\"pyodbc_conn\"])\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            LoanAccountTransactionIdentityId INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            LoanAccountTransactionId INT NOT NULL,\n",
    "            SourceId INT NOT NULL DEFAULT 8,\n",
    "            TransactionDateId INT NOT NULL,\n",
    "            TransactionDate AS CONVERT(DATE, CAST(TransactionDateId AS CHAR(8)), 112) PERSISTED,\n",
    "            ValueDateId INT NOT NULL,\n",
    "            ValueDate AS CONVERT(DATE, CAST(TransactionDateId AS CHAR(8)), 112) PERSISTED,\n",
    "            EntryDateID INT NOT NULL,\n",
    "            EntryDate AS CONVERT(DATE, CAST(EntryDateID AS CHAR(8)), 112) PERSISTED,\n",
    "            LoanAccountId INT NOT NULL,\n",
    "            TransactionTypeId INT NOT NULL,\n",
    "            TransactionStatus NVARCHAR(50),\n",
    "            RectifyStatus NVARCHAR(50),\n",
    "            TransactionCurrencyId INT NOT NULL,\n",
    "            TransactionAmount DECIMAL(12,2) NOT NULL,\n",
    "            TransactionAmountSEK DECIMAL(12,5) DEFAULT 0,\n",
    "            CounterpartClearingNumber NVARCHAR(50),\n",
    "            CounterPartBic NVARCHAR(50),\n",
    "            CounterPartIban NVARCHAR(50),\n",
    "            TransactionReference NVARCHAR(100),\n",
    "            ExchangeRateId INT NOT NULL,\n",
    "            TransactionText NVARCHAR(255),\n",
    "            AccountServicerReference NVARCHAR(100),\n",
    "            CounterPartId INT,\n",
    "            CounterPartAccountNumber NVARCHAR(50),\n",
    "            CounterPartBankName NVARCHAR(100),\n",
    "            TransactionDateTime DATETIME2,\n",
    "            IsDirectDebit INT,\n",
    "            GLAccount NVARCHAR(50),\n",
    "            EventName NVARCHAR(100),\n",
    "            InvoiceId INT,\n",
    "            CreatedDate DATETIME DEFAULT GETDATE(),\n",
    "            UpdatedDate DATE DEFAULT GETDATE(),\n",
    "            UpdatedTime TIME DEFAULT GETDATE()\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_sql)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(f\"Table {table_name} created successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_csv_data(csv_path, schema):\n",
    "    \"\"\"Load and clean CSV data\"\"\"\n",
    "    try:\n",
    "        df = spark.read.option(\"delimiter\", \";\") \\\n",
    "            .csv(csv_path, header=True, schema=schema)\n",
    "        print(f\"Loaded {df.count()} records from {csv_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_existing_data(table_name, file_key):\n",
    "    \"\"\"Load existing data from database or parquet\"\"\"\n",
    "    try:\n",
    "        # Try database first\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", CONFIG[\"db_url\"]) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "        print(f\"Loaded existing data from database table {table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from database: {e}\")\n",
    "\n",
    "        # Fallback to parquet\n",
    "        parquet_path = CONFIG[\"files\"][file_key][\"parquet\"]\n",
    "        if Path(parquet_path).exists():\n",
    "            try:\n",
    "                df = spark.read.parquet(parquet_path)\n",
    "                print(f\"Loaded existing data from parquet {parquet_path}\")\n",
    "                return df\n",
    "            except Exception as pe:\n",
    "                print(f\"Could not load from parquet: {pe}\")\n",
    "        return None\n",
    "\n",
    "print(\"Utility functions loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a4575fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def clean_account_data(df):\n",
    "    \"\"\"Clean and transform account data\"\"\"\n",
    "    # Convert European decimal format columns\n",
    "    decimal_columns = [\n",
    "        \"CurrentInstallmentAmount\",\n",
    "        \"CurrentInvoiceFee\"\n",
    "    ]\n",
    "\n",
    "    # Convert decimals first\n",
    "    converted_df = convert_european_decimals(df, decimal_columns)\n",
    "\n",
    "    cleaned_df = converted_df \\\n",
    "        .withColumn(\"CancelledDateId\",\n",
    "                   when(col(\"CancelledDateId\") == -1, None)\n",
    "                   .otherwise(col(\"CancelledDateId\")))\n",
    "\n",
    "    # Deduplication\n",
    "    window_spec = Window.partitionBy(\"LoanAccountId\").orderBy(col(\"OpenDate\").desc())\n",
    "    deduped_df = cleaned_df \\\n",
    "        .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "        .filter(\"row_num = 1\") \\\n",
    "        .drop(\"row_num\")\n",
    "\n",
    "    print(f\"Cleaned account data: {deduped_df.count()} records after deduplication\")\n",
    "    return deduped_df\n",
    "\n",
    "# def clean_balance_data(df):\n",
    "#     \"\"\"Clean and transform balance data with decimal conversion\"\"\"\n",
    "#     # Convert European decimal format columns\n",
    "#     decimal_columns = [\n",
    "#         \"NetTransactionAmount\",\n",
    "#         \"NetTransactionAmountSek\",\n",
    "#         \"AccruedInterest\",\n",
    "#         \"AccruedInterestSEK\",\n",
    "#         \"Balance\",\n",
    "#         \"BalanceSek\"\n",
    "#     ]\n",
    "\n",
    "#     # Convert decimals first\n",
    "#     converted_df = convert_european_decimals(df, decimal_columns)\n",
    "\n",
    "#     # Clean other columns\n",
    "#     cleaned_df = converted_df \\\n",
    "#         .withColumn(\"PrecedingId\",\n",
    "#                    when(col(\"PrecedingId\") == -1, None)\n",
    "#                    .otherwise(col(\"PrecedingId\")))\n",
    "\n",
    "#     # Deduplication based on LoanAccountId and BalanceDateId\n",
    "#     window_spec = Window.partitionBy(\"LoanAccountId\", \"BalanceDateId\").orderBy(col(\"LoanAccountBalanceId\").desc())\n",
    "#     deduped_df = cleaned_df \\\n",
    "#         .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "#         .filter(\"row_num = 1\") \\\n",
    "#         .drop(\"row_num\")\n",
    "\n",
    "#     print(f\"Cleaned balance data: {deduped_df.count()} records after deduplication\")\n",
    "#     return deduped_df\n",
    "\n",
    "# def clean_transaction_data(df):\n",
    "#     \"\"\"Clean and transform transaction data with decimal conversion\"\"\"\n",
    "#     # Convert European decimal format columns\n",
    "#     decimal_columns = [\n",
    "#         \"TransactionAmount\",\n",
    "#         \"TransactionAmountSEK\"\n",
    "#     ]\n",
    "\n",
    "#     # Convert decimals first\n",
    "#     converted_df = convert_european_decimals(df, decimal_columns)\n",
    "\n",
    "#     # Clean other columns\n",
    "#     cleaned_df = converted_df \\\n",
    "#         .withColumn(\"CounterPartId\",\n",
    "#                    when(col(\"CounterPartId\") == -1, None)\n",
    "#                    .otherwise(col(\"CounterPartId\"))) \\\n",
    "#         .withColumn(\"InvoiceId\",\n",
    "#                    when(col(\"InvoiceId\") == -1, None)\n",
    "#                    .otherwise(col(\"InvoiceId\")))\n",
    "\n",
    "#     # Deduplication based on transaction_id\n",
    "#     window_spec = Window.partitionBy(\"LoanAccountTransactionId\").orderBy(col(\"TransactionDateId\").desc())\n",
    "#     deduped_df = cleaned_df \\\n",
    "#         .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "#         .filter(\"row_num = 1\") \\\n",
    "#         .drop(\"row_num\")\n",
    "\n",
    "#     print(f\"Cleaned transaction data: {deduped_df.count()} records after deduplication\")\n",
    "#     return deduped_df\n",
    "\n",
    "# def add_metadata_columns(df):\n",
    "#     \"\"\"Add metadata columns (timestamps)\"\"\"\n",
    "#     ts = from_utc_timestamp(current_timestamp(), \"Europe/Stockholm\")\n",
    "\n",
    "#     return df \\\n",
    "#         .withColumn(\"CreatedDate\", ts) \\\n",
    "#         .withColumn(\"UpdatedDate\", date_format(ts, \"yyyy-MM-dd\")) \\\n",
    "#         .withColumn(\"UpdatedTime\", date_format(ts, \"HH:mm:ss\"))\n",
    "\n",
    "def prepare_for_initial_load(df, identity_col, sort_col):\n",
    "    \"\"\"Prepare DataFrame for initial load (add identity column)\"\"\"\n",
    "    window_spec = Window.orderBy(sort_col)\n",
    "    df_with_id = df.withColumn(identity_col, row_number().over(window_spec))\n",
    "\n",
    "    # Reorder columns\n",
    "    columns = [identity_col] + [col for col in df_with_id.columns if col != identity_col]\n",
    "    return df_with_id.select(columns)\n",
    "\n",
    "# def detect_changes(new_df, existing_df, compare_cols):\n",
    "#     \"\"\"Detect changes between new and existing data\"\"\"\n",
    "#     new_business_df = new_df.select(compare_cols)\n",
    "#     old_business_df = existing_df.select(compare_cols)\n",
    "\n",
    "#     # Find differences\n",
    "#     diff_df = new_business_df.exceptAll(old_business_df)\n",
    "\n",
    "#     # Get full records for changed data\n",
    "#     changed_df = new_df.join(diff_df, on=compare_cols, how=\"inner\")\n",
    "#     return changed_df, compare_cols\n",
    "\n",
    "# print(\"Data processing functions loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ee0d3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define account comparison columns (business keys)\n",
    "compare_cols_acc = [\n",
    "    \"LoanAccountId\", \"SourceId\", \"AccountNumber\", \"IBAN\", \"BBAN\",\n",
    "    \"AccountCurrencyId\", \"AccountCurrency\", \"OrganizationId\", \"OrganizationName\",\n",
    "    \"ChannelID\", \"BrokerId\", \"OpenDateId\", \"OpenDate\", \"CancelledDateId\",\n",
    "    \"CancelledDate\", \"ValueDate\", \"MaturityDate\", \"ProductId\", \"Product\",\n",
    "    \"InvoiceDay\", \"CurrentInstallmentAmount\", \"CurrentInvoiceFee\",\n",
    "    \"RepaymentRate\", \"NextInvoiceDate\", \"CalculatedMaturityDate\"\n",
    "]\n",
    "\n",
    "# Enhanced Upsert Functions with Better Error Handling\n",
    "def perform_comprehensive_upsert(new_df, existing_df, table_name, parquet_path, key_cols, compare_cols):\n",
    "    \"\"\"\n",
    "    Comprehensive upsert that handles both parquet and database updates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting comprehensive upsert for {table_name}...\")\n",
    "        \n",
    "        # 1. Detect changes\n",
    "        if existing_df is not None:\n",
    "            # Find new and changed records\n",
    "            new_business_df = new_df.select(compare_cols)\n",
    "            old_business_df = existing_df.select(compare_cols)\n",
    "            \n",
    "            # Get records that are different (new or changed)\n",
    "            diff_df = new_business_df.exceptAll(old_business_df)\n",
    "            changed_df = new_df.join(diff_df, on=compare_cols, how=\"inner\")\n",
    "            \n",
    "            changed_count = changed_df.count()\n",
    "            print(f\"Found {changed_count} new/changed records\")\n",
    "            \n",
    "            if changed_count == 0:\n",
    "                print(\"No changes detected. Skipping upsert.\")\n",
    "                return True\n",
    "                \n",
    "        else:\n",
    "            # No existing data - treat as initial load\n",
    "            changed_df = new_df\n",
    "            changed_count = changed_df.count()\n",
    "            print(f\"Initial load: {changed_count} records\")\n",
    "        \n",
    "        # 2. Update Parquet (append new/changed records)\n",
    "        if existing_df is not None:\n",
    "            # For existing data, append only changes\n",
    "            changed_df.write.mode(\"append\").parquet(parquet_path)\n",
    "            print(f\"Appended {changed_count} records to parquet\")\n",
    "        else:\n",
    "            # Initial load\n",
    "            changed_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "            print(f\"Created parquet with {changed_count} records\")\n",
    "        \n",
    "        # 3. Update Database using MERGE\n",
    "        perform_database_merge(changed_df, table_name, key_cols)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comprehensive upsert failed for {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def perform_database_merge(df, table_name, key_cols):\n",
    "    \"\"\"\n",
    "    Perform database MERGE operation for upsert\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_table = f\"{table_name}_staging\"\n",
    "        \n",
    "        # Remove identity columns for database insert\n",
    "        identity_cols = [col for col in df.columns if \"IdentityId\" in col or \"Identity\" in col]\n",
    "        db_df = df\n",
    "        for id_col in identity_cols:\n",
    "            if id_col in db_df.columns:\n",
    "                db_df = db_df.drop(id_col)\n",
    "\n",
    "        # Write to staging table\n",
    "        db_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", CONFIG[\"db_url\"]) \\\n",
    "            .option(\"dbtable\", temp_table) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        \n",
    "        # Execute MERGE statement\n",
    "        conn = pyodbc.connect(CONFIG[\"pyodbc_conn\"])\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Build key matching condition\n",
    "        key_conditions = \" AND \".join([f\"target.{key} = source.{key}\" for key in key_cols])\n",
    "        \n",
    "        # Use only compare_cols (exclude audit fields and keys)\n",
    "        audit_cols = [\"CreatedDate\", \"UpdatedDate\", \"UpdatedTime\"]\n",
    "        update_cols = [col for col in compare_cols_acc if col not in key_cols and col not in audit_cols]\n",
    "        \n",
    "        # Build update set clause (exclude audit fields)\n",
    "        update_set = \", \".join([f\"target.{col} = source.{col}\" for col in update_cols])\n",
    "\n",
    "        # Build insert columns and values (include audit fields, but use GETDATE() for them)\n",
    "        insert_cols = key_cols + update_cols + audit_cols\n",
    "        insert_vals = [f\"source.{col}\" for col in key_cols + update_cols] + [\n",
    "            \"GETDATE()\",           # CreatedDate\n",
    "            \"CONVERT(DATE, GETDATE())\",  # UpdatedDate\n",
    "            \"CONVERT(TIME, GETDATE())\",  # UpdatedTime\n",
    "        ]\n",
    "\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE {table_name} AS target\n",
    "        USING {temp_table} AS source\n",
    "        ON {key_conditions}\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET {update_set},\n",
    "                    UpdatedDate = CONVERT(DATE, GETDATE()),\n",
    "                    UpdatedTime = CONVERT(TIME, GETDATE())\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({', '.join(insert_cols)})\n",
    "            VALUES ({', '.join(insert_vals)});\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"Executing MERGE for {table_name}...\")\n",
    "        cursor.execute(merge_sql)\n",
    "        rows_affected = cursor.rowcount\n",
    "        conn.commit()\n",
    "        \n",
    "        # Clean up staging table\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Database MERGE completed: {rows_affected} rows affected\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database MERGE failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Enhanced ETL Functions with Comprehensive Upsert\n",
    "def run_account_etl_enhanced():\n",
    "    \"\"\"Enhanced ETL process for account data with comprehensive upsert\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING ENHANCED ACCOUNT ETL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_path = CONFIG[\"files\"][\"account\"][\"csv\"]\n",
    "    raw_df = load_csv_data(csv_path, account_schema)\n",
    "    \n",
    "    if raw_df is None:\n",
    "        print(\"Failed to load account CSV data.\")\n",
    "        return False\n",
    "    \n",
    "    # Clean data\n",
    "    cleaned_df = clean_account_data(raw_df)\n",
    "    cleaned_df = add_metadata_columns(cleaned_df)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_name = CONFIG[\"files\"][\"account\"][\"table\"]\n",
    "    parquet_path = CONFIG[\"files\"][\"account\"][\"parquet\"]\n",
    "    table_exists = check_table_exists(table_name)\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(f\"Creating new table {table_name}...\")\n",
    "        if not create_account_table(table_name):\n",
    "            return False\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_df = load_existing_data(table_name, \"account\") if table_exists else None\n",
    "    \n",
    "    # Add identity column for new records\n",
    "    if existing_df is not None:\n",
    "        # Get max identity value\n",
    "        max_id_result = existing_df.agg({\"LoanAccountIdentityId\": \"max\"}).collect()[0][0]\n",
    "        max_id = max_id_result if max_id_result is not None else 0\n",
    "        \n",
    "        window_spec = Window.orderBy(\"LoanAccountId\")\n",
    "        final_df = cleaned_df.withColumn(\"LoanAccountIdentityId\", \n",
    "                                       row_number().over(window_spec) + max_id)\n",
    "    else:\n",
    "        # Initial load\n",
    "        final_df = prepare_for_initial_load(cleaned_df, \"LoanAccountIdentityId\", \"LoanAccountId\")\n",
    "    \n",
    "    # Perform comprehensive upsert\n",
    "    success = perform_comprehensive_upsert(\n",
    "        new_df=final_df,\n",
    "        existing_df=existing_df,\n",
    "        table_name=table_name,\n",
    "        parquet_path=parquet_path,\n",
    "        key_cols=[\"LoanAccountId\"],\n",
    "        compare_cols=compare_cols_acc\n",
    "    )\n",
    "    \n",
    "    print(f\"Account ETL completed: {'SUCCESS' if success else 'FAILED'}\")\n",
    "    return success\n",
    "\n",
    "def run_balance_etl_enhanced():\n",
    "    \"\"\"Enhanced ETL process for balance data with comprehensive upsert\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING ENHANCED BALANCE ETL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_path = CONFIG[\"files\"][\"balance\"][\"csv\"]\n",
    "    raw_df = load_csv_data(csv_path, balance_schema)\n",
    "    \n",
    "    if raw_df is None:\n",
    "        print(\"Failed to load balance CSV data.\")\n",
    "        return False\n",
    "    \n",
    "    # Clean data\n",
    "    cleaned_df = clean_balance_data(raw_df)\n",
    "    cleaned_df = add_metadata_columns(cleaned_df)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_name = CONFIG[\"files\"][\"balance\"][\"table\"]\n",
    "    parquet_path = CONFIG[\"files\"][\"balance\"][\"parquet\"]\n",
    "    table_exists = check_table_exists(table_name)\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(f\"Creating new table {table_name}...\")\n",
    "        if not create_balance_table(table_name):\n",
    "            return False\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_df = load_existing_data(table_name, \"balance\") if table_exists else None\n",
    "    \n",
    "    # Add identity column for new records\n",
    "    if existing_df is not None:\n",
    "        max_id_result = existing_df.agg({\"LoanAccountBalanceIdentityId\": \"max\"}).collect()[0][0]\n",
    "        max_id = max_id_result if max_id_result is not None else 0\n",
    "        \n",
    "        window_spec = Window.orderBy(\"LoanAccountBalanceId\")\n",
    "        final_df = cleaned_df.withColumn(\"LoanAccountBalanceIdentityId\", \n",
    "                                       row_number().over(window_spec) + max_id)\n",
    "    else:\n",
    "        final_df = prepare_for_initial_load(cleaned_df, \"LoanAccountBalanceIdentityId\", \"LoanAccountBalanceId\")\n",
    "    \n",
    "    # Define comparison columns\n",
    "    compare_cols = [\n",
    "        \"LoanAccountBalanceId\", \"SourceId\", \"BalanceDateId\", \"LoanAccountId\",\n",
    "        \"ProductId\", \"AccountCurrencyId\", \"AccountStatusId\", \"NumOfTransactions\",\n",
    "        \"NetTransactionAmount\", \"NetTransactionAmountSek\", \"AccruedInterest\",\n",
    "        \"AccruedInterestSEK\", \"Balance\", \"BalanceSek\", \"LTV\", \"PrecedingId\"\n",
    "    ]\n",
    "    \n",
    "    # Perform comprehensive upsert\n",
    "    success = perform_comprehensive_upsert(\n",
    "        new_df=final_df,\n",
    "        existing_df=existing_df,\n",
    "        table_name=table_name,\n",
    "        parquet_path=parquet_path,\n",
    "        key_cols=[\"LoanAccountBalanceId\"],\n",
    "        compare_cols=compare_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"Balance ETL completed: {'SUCCESS' if success else 'FAILED'}\")\n",
    "    return success\n",
    "\n",
    "def run_transaction_etl_enhanced():\n",
    "    \"\"\"Enhanced ETL process for transaction data with comprehensive upsert\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING ENHANCED TRANSACTION ETL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_path = CONFIG[\"files\"][\"transactions\"][\"csv\"]\n",
    "    raw_df = load_csv_data(csv_path, transaction_schema)\n",
    "    \n",
    "    if raw_df is None:\n",
    "        print(\"Failed to load transaction CSV data.\")\n",
    "        return False\n",
    "    \n",
    "    # Clean data\n",
    "    cleaned_df = clean_transaction_data(raw_df)\n",
    "    cleaned_df = add_metadata_columns(cleaned_df)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_name = CONFIG[\"files\"][\"transactions\"][\"table\"]\n",
    "    parquet_path = CONFIG[\"files\"][\"transactions\"][\"parquet\"]\n",
    "    table_exists = check_table_exists(table_name)\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(f\"Creating new table {table_name}...\")\n",
    "        if not create_transaction_table(table_name):\n",
    "            return False\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_df = load_existing_data(table_name, \"transactions\") if table_exists else None\n",
    "    \n",
    "    # Add identity column for new records\n",
    "    if existing_df is not None:\n",
    "        max_id_result = existing_df.agg({\"LoanAccountTransactionIdentityId\": \"max\"}).collect()[0][0]\n",
    "        max_id = max_id_result if max_id_result is not None else 0\n",
    "        \n",
    "        window_spec = Window.orderBy(\"LoanAccountTransactionId\")\n",
    "        final_df = cleaned_df.withColumn(\"LoanAccountTransactionIdentityId\", \n",
    "                                       row_number().over(window_spec) + max_id)\n",
    "    else:\n",
    "        final_df = prepare_for_initial_load(cleaned_df, \"LoanAccountTransactionIdentityId\", \"LoanAccountTransactionId\")\n",
    "    \n",
    "    # Define comparison columns\n",
    "    compare_cols = [\n",
    "        \"LoanAccountTransactionId\", \"SourceId\", \"TransactionDateId\", \"ValueDateId\",\n",
    "        \"EntryDateID\", \"LoanAccountId\", \"TransactionTypeId\", \"TransactionStatus\",\n",
    "        \"RectifyStatus\", \"TransactionCurrencyId\", \"TransactionAmount\", \"TransactionAmountSEK\",\n",
    "        \"CounterpartClearingNumber\", \"CounterPartBic\", \"CounterPartIban\", \"TransactionReference\",\n",
    "        \"ExchangeRateId\", \"TransactionText\", \"AccountServicerReference\", \"CounterPartId\",\n",
    "        \"CounterPartAccountNumber\", \"CounterPartBankName\", \"TransactionDateTime\",\n",
    "        \"IsDirectDebit\", \"GLAccount\", \"EventName\", \"InvoiceId\"\n",
    "    ]\n",
    "    \n",
    "    # Perform comprehensive upsert\n",
    "    success = perform_comprehensive_upsert(\n",
    "        new_df=final_df,\n",
    "        existing_df=existing_df,\n",
    "        table_name=table_name,\n",
    "        parquet_path=parquet_path,\n",
    "        key_cols=[\"LoanAccountTransactionId\"],\n",
    "        compare_cols=compare_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"Transaction ETL completed: {'SUCCESS' if success else 'FAILED'}\")\n",
    "    return success\n",
    "\n",
    "# Enhanced Full ETL Process\n",
    "def run_enhanced_etl_process():\n",
    "    \"\"\"Run enhanced ETL for all tables with comprehensive upsert\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING ENHANCED FULL ETL PROCESS WITH COMPREHENSIVE UPSERT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    success_count = 0\n",
    "    total_tables = 3\n",
    "    \n",
    "    # Run Enhanced Account ETL\n",
    "    if run_account_etl_enhanced():\n",
    "        success_count += 1\n",
    "    \n",
    "    # Run Enhanced Balance ETL\n",
    "    # if run_balance_etl_enhanced():\n",
    "    #     success_count += 1\n",
    "    \n",
    "    # # Run Enhanced Transaction ETL\n",
    "    # if run_transaction_etl_enhanced():\n",
    "    #     success_count += 1\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"ENHANCED ETL PROCESS COMPLETED\")\n",
    "    print(f\"SUCCESS: {success_count}/{total_tables} tables processed successfully\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return success_count == total_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d1498554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING ENHANCED FULL ETL PROCESS WITH COMPREHENSIVE UPSERT\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "STARTING ENHANCED ACCOUNT ETL\n",
      "==================================================\n",
      "Loaded 499 records from data/Raw_Loan_Account.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned account data: 499 records after deduplication\n",
      "Loaded existing data from database table Loan_Account_cleansed\n",
      "Starting comprehensive upsert for Loan_Account_cleansed...\n",
      "Found 0 new/changed records\n",
      "No changes detected. Skipping upsert.\n",
      "Account ETL completed: SUCCESS\n",
      "\n",
      "================================================================================\n",
      "ENHANCED ETL PROCESS COMPLETED\n",
      "SUCCESS: 1/3 tables processed successfully\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute Enhanced ETL Process\n",
    "run_enhanced_etl_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46fec057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LoanAccountTransactionIdentityId: integer (nullable = true)\n",
      " |-- LoanAccountTransactionId: integer (nullable = true)\n",
      " |-- SourceId: integer (nullable = true)\n",
      " |-- TransactionDateId: integer (nullable = true)\n",
      " |-- ValueDateId: integer (nullable = true)\n",
      " |-- EntryDateID: integer (nullable = true)\n",
      " |-- LoanAccountId: integer (nullable = true)\n",
      " |-- TransactionTypeId: integer (nullable = true)\n",
      " |-- TransactionStatus: string (nullable = true)\n",
      " |-- RectifyStatus: string (nullable = true)\n",
      " |-- TransactionCurrencyId: integer (nullable = true)\n",
      " |-- TransactionAmount: double (nullable = true)\n",
      " |-- TransactionAmountSEK: double (nullable = true)\n",
      " |-- CounterpartClearingNumber: string (nullable = true)\n",
      " |-- CounterPartBic: string (nullable = true)\n",
      " |-- CounterPartIban: string (nullable = true)\n",
      " |-- TransactionReference: string (nullable = true)\n",
      " |-- ExchangeRateId: integer (nullable = true)\n",
      " |-- TransactionText: string (nullable = true)\n",
      " |-- AccountServicerReference: string (nullable = true)\n",
      " |-- CounterPartId: integer (nullable = true)\n",
      " |-- CounterPartAccountNumber: string (nullable = true)\n",
      " |-- CounterPartBankName: string (nullable = true)\n",
      " |-- TransactionDateTime: timestamp (nullable = true)\n",
      " |-- IsDirectDebit: integer (nullable = true)\n",
      " |-- GLAccount: string (nullable = true)\n",
      " |-- EventName: string (nullable = true)\n",
      " |-- InvoiceId: integer (nullable = true)\n",
      " |-- CreatedDate: timestamp (nullable = true)\n",
      " |-- UpdatedDate: date (nullable = true)\n",
      " |-- UpdatedTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(r\"C:\\Users\\anton\\Local Data\\BI Local\\7.Python\\Py-class\\SQL-ETL-2\\data\\Loan_Transaction_cleansed.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e692c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------+--------+-----------------+-----------+-----------+-------------+-----------------+-----------------+-------------+---------------------+-----------------+--------------------+-------------------------+--------------+---------------+--------------------+--------------+---------------+------------------------+-------------+------------------------+-------------------+-------------------+-------------+---------+---------+---------+--------------------+-----------+-----------+\n",
      "|LoanAccountTransactionIdentityId|LoanAccountTransactionId|SourceId|TransactionDateId|ValueDateId|EntryDateID|LoanAccountId|TransactionTypeId|TransactionStatus|RectifyStatus|TransactionCurrencyId|TransactionAmount|TransactionAmountSEK|CounterpartClearingNumber|CounterPartBic|CounterPartIban|TransactionReference|ExchangeRateId|TransactionText|AccountServicerReference|CounterPartId|CounterPartAccountNumber|CounterPartBankName|TransactionDateTime|IsDirectDebit|GLAccount|EventName|InvoiceId|         CreatedDate|UpdatedDate|UpdatedTime|\n",
      "+--------------------------------+------------------------+--------+-----------------+-----------+-----------+-------------+-----------------+-----------------+-------------+---------------------+-----------------+--------------------+-------------------------+--------------+---------------+--------------------+--------------+---------------+------------------------+-------------+------------------------+-------------------+-------------------+-------------+---------+---------+---------+--------------------+-----------+-----------+\n",
      "|                               1|                 1789263|       8|         20230519|   20230710|   20230710|          484|             1870|             NULL|         NULL|                   49|          -486.65|          -5491.4316|                     NULL|          NULL|           NULL|00446003474-49352453|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789262|       8|         20230519|   20230710|   20230710|          417|             1870|             NULL|         NULL|                   49|          -126.29|          -1425.0753|                     NULL|          NULL|           NULL|00446003474-49342940|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789261|       8|         20230519|   20230710|   20230710|          382|             1870|             NULL|         NULL|                   49|           -49.89|          -562.96624|                     NULL|          NULL|           NULL|00446003474-49351392|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789260|       8|         20230519|   20230710|   20230710|          377|             1870|             NULL|         NULL|                   49|          -647.28|         -7304.00461|                     NULL|          NULL|           NULL|00446003474-49351222|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789259|       8|         20230519|   20230710|   20230710|          377|             1821|             NULL|         NULL|                   49|        -48583.71|        -548225.8712|                     NULL|          NULL|           NULL|00446003474-49351226|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789258|       8|         20230519|   20230710|   20230710|          217|             1870|             NULL|         NULL|                   49|          -257.17|         -2901.94486|                     NULL|          NULL|           NULL|00446003474-49332483|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789257|       8|         20230519|   20230710|   20230710|          217|             1769|             NULL|         NULL|                   49|             -5.0|           -56.42075|                     NULL|          NULL|           NULL|00446003474-49332481|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789256|       8|         20230519|   20230710|   20230710|           24|             1870|             NULL|         NULL|                   49|          -396.85|         -4478.11493|                     NULL|          NULL|           NULL|00446003474-49338404|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1789255|       8|         20230519|   20230710|   20230710|           24|             1769|             NULL|         NULL|                   49|            -37.7|          -425.41246|                     NULL|          NULL|           NULL|00446003474-49338402|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "|                               1|                 1761551|       8|         20230517|   20230701|   20230701|          355|             1870|             NULL|         NULL|                   49|           -72.22|          -814.94131|                     NULL|          NULL|           NULL|00446003473-49329094|          2910|           NULL|                    NULL|         NULL|                    NULL|               NULL|               NULL|         NULL|     NULL|     NULL|     NULL|2025-06-06 16:18:...| 2025-06-06|   16:18:24|\n",
      "+--------------------------------+------------------------+--------+-----------------+-----------+-----------+-------------+-----------------+-----------------+-------------+---------------------+-----------------+--------------------+-------------------------+--------------+---------------+--------------------+--------------+---------------+------------------------+-------------+------------------------+-------------------+-------------------+-------------+---------+---------+---------+--------------------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1475"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(col(\"LoanAccountTransactionId\").desc()).limit(10).show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "Could not load from database table Loan_Transaction_cleansed: An error occurred while calling o90.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Invalid object name 'Loan_Transaction_cleansed'.\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:276)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1787)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:688)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:607)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4700)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:321)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:253)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:521)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:70)\n",
      "\tat scala.util.Using$.resource(Using.scala:296)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$1(JDBCRDD.scala:68)\n",
      "\tat scala.util.Using$.resource(Using.scala:296)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:67)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:243)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:276)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1787)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:688)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:607)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4700)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:321)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:253)\n",
      "\t\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:521)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:70)\n",
      "\t\tat scala.util.Using$.resource(Using.scala:296)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$1(JDBCRDD.scala:68)\n",
      "\t\tat scala.util.Using$.resource(Using.scala:296)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:67)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:243)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:38)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 21 more\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     15\u001b[39m df = load_database_data(\u001b[33m\"\u001b[39m\u001b[33mLoan_Transaction_cleansed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprintSchema\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "def load_database_data(table_name):\n",
    "    \"\"\"Load data from database table\"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", CONFIG[\"db_url\"]) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "        count = df.count()\n",
    "        print(f\"        Loaded {count} records from database table {table_name}\")\n",
    "        return df if count > 0 else None\n",
    "    except Exception as e:\n",
    "        print(f\"        \\nCould not load from database table {table_name}: {e}\")\n",
    "        return None\n",
    "df = load_database_data(\"Loan_Transaction_cleansed\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9566f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
