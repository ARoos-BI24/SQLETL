{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d55ff0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['JAVA_HOME'] = 'C:/Users/anton/.conda/envs/conda-env/Library/lib/jvm'\n",
    "# os.environ['HADOOP_HOME'] = 'C:/Hadoop/hadoop-3.3.6'\n",
    "# os.environ['PATH'] += ';C:/Hadoop/hadoop-3.3.6/bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d444883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    when, col, row_number, current_timestamp, date_format, from_utc_timestamp\n",
    "    )\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ETL_Assignment2\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        r\"C:\\Spark\\sqljdbc_12.10\\enu\\jars\\mssql-jdbc-12.10.0.jre8.jar\"\n",
    "        ) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "191b4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Define file names and paths\n",
    "account_file_name = \"Loan_Account\"\n",
    "balance_file_name = \"Loan_Balance\"\n",
    "transaction_file_name = \"Loan_Transaction\"\n",
    "\n",
    "raw_account = f\"Raw_{account_file_name}\"\n",
    "raw_balance = f\"Raw_{balance_file_name}\"\n",
    "raw_transaction = f\"Raw_{transaction_file_name}\"\n",
    "\n",
    "account_csv_file = f\"{raw_account}.csv\"\n",
    "balance_csv_file = f\"{raw_balance}.csv\"\n",
    "transaction_csv_file = f\"{raw_transaction}.csv\"\n",
    "\n",
    "base_path = \"data\"\n",
    "\n",
    "account_csv_path = f\"{base_path}/{account_csv_file}\"\n",
    "balance_csv_path = f\"{base_path}/{balance_csv_file}\"\n",
    "transaction_csv_path = f\"{base_path}/{transaction_csv_file}\"\n",
    "\n",
    "cleansed_account = f\"{account_file_name}_cleansed\"\n",
    "cleansed_balance = f\"{balance_file_name}_cleansed\"\n",
    "cleansed_transactions = f\"{transaction_file_name}_cleansed\"\n",
    "\n",
    "account_parquet_file = f\"{cleansed_account}.parquet\"\n",
    "balance_parquet_file = f\"{cleansed_balance}.parquet\"\n",
    "transaction_parquet_file = f\"{cleansed_transactions}.parquet\"\n",
    "\n",
    "account_cleansed_parquet_path = f\"{base_path}/{account_parquet_file}\"\n",
    "balance_cleansed_parquet_path = f\"{base_path}/{balance_parquet_file}\"\n",
    "transaction_cleansed_parquet_path = f\"{base_path}/{transaction_parquet_file}\"\n",
    "\n",
    "# Database connection string\n",
    "url_conn = \"jdbc:sqlserver://PC-W11:1433;databaseName=ETL_Assignment2;\" \\\n",
    "\"user=admin;password=sql;encrypt=false;trustServerCertificate=true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aeecdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_db_source(spark, url, table_name, schema=None):\n",
    "    \"\"\"Load source table from database.\"\"\"\n",
    "    try:\n",
    "        # Try database\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "\n",
    "        print(f\"Loaded {table_name} from database\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Database load for {table_name} failed: \\n{e}\\n\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48160698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "account_schema = StructType([\n",
    "    StructField(\"LoanAccountId\", IntegerType(), False),\n",
    "    StructField(\"SourceId\", IntegerType(), False),\n",
    "    StructField(\"AccountNumber\", StringType(), False),\n",
    "    StructField(\"IBAN\", StringType(), True),\n",
    "    StructField(\"BBAN\", StringType(), True),\n",
    "    StructField(\"AccountCurrencyId\", IntegerType(), False),\n",
    "    StructField(\"AccountCurrency\", StringType(), False),\n",
    "    StructField(\"OrganizationId\", IntegerType(), False),\n",
    "    StructField(\"OrganizationName\", StringType(), False),\n",
    "    StructField(\"ChannelID\", IntegerType(), True),\n",
    "    StructField(\"BrokerId\", IntegerType(), False),\n",
    "    StructField(\"OpenDateId\", IntegerType(), False),\n",
    "    StructField(\"OpenDate\", DateType(), False),\n",
    "    StructField(\"CancelledDateId\", IntegerType(), True),\n",
    "    StructField(\"CancelledDate\", DateType(), True),\n",
    "    StructField(\"ValueDate\", DateType(), False),\n",
    "    StructField(\"MaturityDate\", DateType(), True),\n",
    "    StructField(\"ProductId\", IntegerType(), False),\n",
    "    StructField(\"Product\", StringType(), False),\n",
    "    StructField(\"InvoiceDay\", IntegerType(), False),\n",
    "    StructField(\"CurrentInstallmentAmount\", DecimalType(12,2), False),\n",
    "    StructField(\"CurrentInvoiceFee\", DecimalType(6,2), False),\n",
    "    StructField(\"RepaymentRate\", StringType(), True),  # NVARCHAR(50)\n",
    "    StructField(\"NextInvoiceDate\", DateType(), False),\n",
    "    StructField(\"CalculatedMaturityDate\", DateType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfab274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(spark, csv_file_path, schema):\n",
    "    \"\"\"Load data from CSV file with optional schema.\"\"\"\n",
    "    print(f\"Loading dataframe for {csv_file_path}, checking schema..\")\n",
    "    try:\n",
    "        if schema:\n",
    "            df = spark.read.option(\"delimiter\", \";\") \\\n",
    "                .csv(csv_file_path, header=True, schema=account_schema)\n",
    "            print(f\"Loaded {csv_file_path} from CSV file with custom schema.\")\n",
    "            return df\n",
    "        else:\n",
    "            df = spark.read.option(\"delimiter\", \";\") \\\n",
    "                .csv(csv_file_path, header=True, inferSchema=True)\n",
    "        print(f\"Loaded {csv_file_path} from CSV file with inferred schema.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_file_path} from CSV: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f67809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe for data/Raw_Loan_Account.csv, checking schema..\n",
      "Loaded data/Raw_Loan_Account.csv from CSV file with custom schema.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_account_source = load_source(\n",
    "        spark, account_csv_path, schema=account_schema\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load {account_csv_path}: \\n{e}\\n\")\n",
    "    df_account_source = None\n",
    "\n",
    "# try:\n",
    "#     df_balance_source = load_source(\n",
    "#         spark, url_conn, raw_balance, balance_csv_path, balance_schema\n",
    "#         )\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to load {raw_balance}: \\n{e}\\n\")\n",
    "#     df_balance_source = None\n",
    "\n",
    "# try:\n",
    "#     df_transaction_source = load_source(\n",
    "#         spark, url_conn, raw_transaction, transaction_csv_path, transaction_schema\n",
    "#         )\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to load {raw_transaction}: \\n{e}\\n\")\n",
    "#     df_transaction_source = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59b09bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_account_source.show()\n",
    "# df_balance_source.show()\n",
    "# df_transactions_source.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ee24248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_account_source.printSchema()\n",
    "# df_balance_source.printSchema()\n",
    "# df_transactions_source.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c70d0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_account_source.describe().show()\n",
    "# df_balance_source.describe().show()\n",
    "# df_transactions_source.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cf41cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing -1 with NULL and Renaming a column\n",
    "account_staging_df = df_account_source \\\n",
    "    .withColumn(\"CancelledDateId\",\n",
    "                when(col(\"CancelledDateId\") == -1,None)\n",
    "                .otherwise(col(\"CancelledDateId\"))\n",
    "                )\n",
    "    # .withColumnRenamed(\"ChannelID\", \"ChannelId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1475d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "account_staging_window_spec = Window.partitionBy(\"LoanAccountId\") \\\n",
    "    .orderBy(account_staging_df[\"OpenDate\"].desc())\n",
    "ranked_account_df = account_staging_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(account_staging_window_spec))\n",
    "latest_account_df = ranked_account_df \\\n",
    "    .filter(\"row_num = 1\").drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e51db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_account_df.filter(\"row_num = 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a651b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CURRENT_TIMESTAMP\n",
    "ts = from_utc_timestamp(current_timestamp(), \"Europe/Stockholm\")\n",
    "\n",
    "# Add CreatedDate, UpdatedDate, UpdatedTime\n",
    "account_with_date_df = latest_account_df \\\n",
    "    .withColumn(\"CreatedDate\", ts) \\\n",
    "    .withColumn(\"UpdatedDate\", date_format(ts, \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"UpdatedTime\", date_format(ts, \"HH:mm:ss\"))\n",
    "\n",
    "# Create a window for row_number (can order by CustomerID or any column)\n",
    "account_window_spec = Window.orderBy(\"LoanAccountId\")\n",
    "\n",
    "# Add LoanAccountIdentityId (1-based index like SQL IDENTITY(1,1))\n",
    "account_new_id_df = account_with_date_df \\\n",
    "    .withColumn(\n",
    "        \"LoanAccountIdentityId\",\n",
    "        row_number().over(account_window_spec)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b17f93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering columns\n",
    "columns = (\n",
    "    [\"LoanAccountIdentityId\"] +\n",
    "    [col for col in account_new_id_df.columns if col !=\n",
    "    \"LoanAccountIdentityId\"]\n",
    "    )\n",
    "\n",
    "account_cleansed_df = account_new_id_df.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97d663a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account_cleansed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5e79261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local save\n",
    "account_cleansed_df.write.mode(\"overwrite\").parquet(account_cleansed_parquet_path)\n",
    "# balance_cleansed_df.write.mode(\"overwrite\").parquet(balance_cleansed_parquet_path)\n",
    "# transaction_cleansed_df.write.mode(\"overwrite\").parquet(transaction_cleansed_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03465800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Schema comparison\n",
    "# schema_1 = account_cleansed_df.schema\n",
    "# schema_2 = df_account_source.schema\n",
    "\n",
    "# if schema_1 == schema_2:\n",
    "#     print(\"Schemas are identical.\")\n",
    "# else:\n",
    "#     print(\"Schemas are different!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78b191e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local file read\n",
    "# df_loan_account = spark.read.parquet(account_cleansed_output_path)\n",
    "\n",
    "# df_loan_account.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17786e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Cleanse data load\n",
    "# try:\n",
    "#     df_account_cleansed_source = load_with_fallback(\n",
    "#         spark, url_conn, cleansed_account, account_parquet\n",
    "#         )\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to load {account_parquet}: \\n{e}\\n\")\n",
    "#     df_account_cleansed_source = None\n",
    "\n",
    "# try:\n",
    "#     df_balance_cleansed_source = load_with_fallback(\n",
    "#         spark, url_conn, cleansed_balance, balance_parquet\n",
    "#         )\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to load {balance_parquet}: \\n{e}\\n\")\n",
    "#     df_balance_cleansed_source = None\n",
    "\n",
    "# try:\n",
    "#     df_transaction_cleansed_source = load_with_fallback(\n",
    "#         spark, url_conn, cleansed_transactions, transaction_parquet\n",
    "#         )\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to load {transaction_parquet}: \\n{e}\\n\")\n",
    "#     df_transaction_cleansed_source = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2781ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cleansed table in SQL Server\n",
    "conn = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=PC-W11;DATABASE=ETL_Assignment2;UID=admin;PWD=sql\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cursor.execute(f\"\"\"\n",
    "    IF OBJECT_ID('{cleansed_account}', 'U') IS NOT NULL\n",
    "        DROP TABLE {cleansed_account};\n",
    "\n",
    "    CREATE TABLE Loan_Account_cleansed (\n",
    "        LoanAccountIdentityId INT PRIMARY KEY,\n",
    "        LoanAccountId INT NOT NULL,\n",
    "        SourceId INT NOT NULL DEFAULT 8,\n",
    "        AccountNumber NVARCHAR(50) NOT NULL,\n",
    "        IBAN NVARCHAR(34),  -- IBAN max length is 34 characters\n",
    "        BBAN NVARCHAR(34),  -- Similar to IBAN\n",
    "        AccountCurrencyId INT NOT NULL,\n",
    "        AccountCurrency NVARCHAR(3) NOT NULL CHECK (LEN(AccountCurrency) = 3),\n",
    "        OrganizationId INT NOT NULL,\n",
    "        OrganizationName NVARCHAR(50) NOT NULL,\n",
    "        ChannelId INT,\n",
    "        BrokerId INT NOT NULL,\n",
    "        OpenDateId INT NOT NULL,\n",
    "        OpenDate DATE NOT NULL,\n",
    "        CancelledDateId INT,\n",
    "        CancelledDate DATE,\n",
    "        ValueDate DATE NOT NULL,\n",
    "        MaturityDate DATE,\n",
    "        ProductId INT NOT NULL,\n",
    "        Product NVARCHAR(30) NOT NULL,\n",
    "        InvoiceDay TINYINT NOT NULL DEFAULT 14, -- Valid day range\n",
    "        CurrentInstallmentAmount DECIMAL(12,2) NOT NULL,\n",
    "        CurrentInvoiceFee DECIMAL (6,2) NOT NULL,\n",
    "        RepaymentRate NVARCHAR(50),\n",
    "        NextInvoiceDate DATE NOT NULL,\n",
    "        CalculatedMaturityDate DATE,\n",
    "        CreatedDate DATETIME DEFAULT GETDATE(),\n",
    "        UpdatedDate DATE DEFAULT GETDATE(),\n",
    "        UpdatedTime TIME DEFAULT GETDATE()\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"CREATE TABLE {cleansed_account} failed: \\n{e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41f798a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing loan account data found. Performing upsert...\n",
      "Loaded Loan_Account_cleansed from parquet fallback\n",
      "Number of new/changed loan account records to append: 0\n",
      "No loan account changes detected.\n",
      "Final loan account dataset contains 499 records.\n"
     ]
    }
   ],
   "source": [
    "# Add missing load_with_fallback function\n",
    "def load_with_fallback(spark, url_conn, table_name, parquet_path):\n",
    "    \"\"\"Load data with fallback from database to parquet\"\"\"\n",
    "    # try:\n",
    "    #     # Try database first\n",
    "    #     df = spark.read.format(\"jdbc\") \\\n",
    "    #         .option(\"url\", url_conn) \\\n",
    "    #         .option(\"dbtable\", table_name) \\\n",
    "    #         .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    #         .load()\n",
    "    #     print(f\"Loaded {table_name} from database\")\n",
    "    #     return df\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Database load failed: {e}\")\n",
    "        # Fallback to parquet\n",
    "    if Path(parquet_path).exists():\n",
    "        try:\n",
    "            df = spark.read.parquet(parquet_path)\n",
    "            print(f\"Loaded {table_name} from parquet fallback\")\n",
    "            return df\n",
    "        except Exception as pe:\n",
    "            print(f\"Parquet load failed: {pe}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Business keys for comparison (exclude timestamps and identity keys)\n",
    "compare_cols = [\n",
    "    \"LoanAccountId\", \"SourceId\", \"AccountNumber\", \"IBAN\", \"BBAN\",\n",
    "    \"AccountCurrencyId\", \"AccountCurrency\", \"OrganizationId\", \"OrganizationName\",\n",
    "    \"ChannelID\", \"BrokerId\", \"OpenDateId\", \"OpenDate\", \"CancelledDateId\",\n",
    "    \"CancelledDate\", \"ValueDate\", \"MaturityDate\", \"ProductId\", \"Product\",\n",
    "    \"InvoiceDay\", \"CurrentInstallmentAmount\", \"CurrentInvoiceFee\",\n",
    "    \"RepaymentRate\", \"NextInvoiceDate\", \"CalculatedMaturityDate\"\n",
    "]\n",
    "\n",
    "parquet_path = Path(account_cleansed_parquet_path)\n",
    "\n",
    "if parquet_path.exists() and any(parquet_path.glob(\"*.parquet\")):\n",
    "    print(\"Existing loan account data found. Performing upsert...\")\n",
    "\n",
    "    # Load existing data with fallback\n",
    "    existing_df = load_with_fallback(\n",
    "        spark, url_conn, cleansed_account, account_cleansed_parquet_path\n",
    "        )\n",
    "\n",
    "    if existing_df is not None:\n",
    "        existing_df.cache()\n",
    "\n",
    "        # Strip down to business columns for exact row-level difference\n",
    "        new_business_df = account_cleansed_df.select(compare_cols)\n",
    "        old_business_df = existing_df.select(compare_cols)\n",
    "\n",
    "        # Get only the truly new/changed rows\n",
    "        diff_df = new_business_df.exceptAll(old_business_df)\n",
    "\n",
    "        # Rejoin with original new input to get full rows\n",
    "        changed_df = account_cleansed_df.join(diff_df, on=compare_cols, how=\"inner\")\n",
    "\n",
    "        # Debug\n",
    "        changed_df.cache()\n",
    "        change_count = changed_df.count()\n",
    "        print(f\"Number of new/changed loan account records to append: {change_count}\")\n",
    "\n",
    "        if change_count > 0:\n",
    "            # Show sample of changes\n",
    "            print(\"Sample of changed records:\")\n",
    "            changed_df.show(5, truncate=False)\n",
    "\n",
    "            # Assign new LoanAccountIdentityId\n",
    "            max_id_result = existing_df.agg({\"LoanAccountIdentityId\": \"max\"}).collect()[0][0]\n",
    "            max_id = max_id_result if max_id_result is not None else 0\n",
    "\n",
    "            window_spec = Window.orderBy(\"LoanAccountId\")\n",
    "            changed_df = changed_df.withColumn(\"LoanAccountIdentityId\", row_number().over(window_spec) + max_id)\n",
    "\n",
    "            # Add timestamps\n",
    "            ts = from_utc_timestamp(current_timestamp(), \"Europe/Stockholm\")\n",
    "            changed_df = changed_df.withColumn(\"CreatedDate\", ts) \\\n",
    "                                   .withColumn(\"UpdatedDate\", date_format(ts, \"yyyy-MM-dd\")) \\\n",
    "                                   .withColumn(\"UpdatedTime\", date_format(ts, \"HH:mm:ss\"))\n",
    "\n",
    "            # Reorder columns to match existing structure\n",
    "            columns = (\n",
    "                [\"LoanAccountIdentityId\"] +\n",
    "                [col for col in changed_df.columns if col != \"LoanAccountIdentityId\"]\n",
    "            )\n",
    "            changed_df = changed_df.select(columns)\n",
    "\n",
    "            # Append only changed rows\n",
    "            changed_df.write.mode(\"append\").parquet(account_cleansed_parquet_path)\n",
    "            print(f\"Appended {change_count} changed loan account rows.\")\n",
    "\n",
    "            # Update account_cleansed_df to reflect the complete dataset\n",
    "            account_cleansed_df = existing_df.union(changed_df)\n",
    "        else:\n",
    "            print(\"No loan account changes detected.\")\n",
    "            account_cleansed_df = existing_df\n",
    "    else:\n",
    "        print(\"Could not load existing data, performing initial load...\")\n",
    "        # Fall through to initial load\n",
    "        parquet_path = Path(\"nonexistent\")  # Force initial load logic\n",
    "\n",
    "if not (parquet_path.exists() and any(parquet_path.glob(\"*.parquet\"))):\n",
    "    print(\"No existing loan account data â€” writing initial dataset.\")\n",
    "\n",
    "    # Just add proper timestamps if not already present\n",
    "    if \"CreatedDate\" not in account_cleansed_df.columns:\n",
    "        ts = from_utc_timestamp(current_timestamp(), \"Europe/Stockholm\")\n",
    "        account_cleansed_df = account_cleansed_df.withColumn(\"CreatedDate\", ts) \\\n",
    "                                               .withColumn(\"UpdatedDate\", date_format(ts, \"yyyy-MM-dd\")) \\\n",
    "                                               .withColumn(\"UpdatedTime\", date_format(ts, \"HH:mm:ss\"))\n",
    "\n",
    "    account_cleansed_df.write.mode(\"overwrite\").parquet(account_cleansed_parquet_path)\n",
    "    print(\"Initial loan account load complete.\")\n",
    "\n",
    "print(f\"Final loan account dataset contains {account_cleansed_df.count()} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0959e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"jdbc:sqlserver://PC-W11:1433;databaseName=ETL_Assignment2;user=admin;password=sql;encrypt=false;trustServerCertificate=true\"\n",
    "\n",
    "# account_cleansed_df.write \\\n",
    "#   .format(\"jdbc\") \\\n",
    "#   .option(\"url\", url) \\\n",
    "#   .option(\"dbtable\", \"Loan_Account_cleansed\") \\\n",
    "#   .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f14cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import airflow\n",
    "# print(airflow.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
